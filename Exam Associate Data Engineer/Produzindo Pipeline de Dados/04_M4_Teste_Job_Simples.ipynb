{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba5aa3d7-e612-4ca9-94e2-a88379de4dd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"data_processamento\", \"2025-01-01\", \"Data de Processamento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fde20564-1fc9-406f-a88e-0dfeb48120e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks notebook source\n",
    "\n",
    " Notebook de Teste Simples para Databricks Job\n",
    "\n",
    " Este notebook foi criado para testar a funcionalidade de um Databricks Job.\n",
    " Ele simula uma tarefa de processamento muito básica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66f19a98-5a4c-449f-a1aa-c121e2152eb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importa a sessão Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"TesteJob\").getOrCreate()\n",
    "\n",
    "# Tenta obter o parâmetro 'data_processamento'\n",
    "try:\n",
    "    data_processamento = dbutils.widgets.get(\"data_processamento\")\n",
    "    print(f\"Parâmetro 'data_processamento' recebido do Job: {data_processamento}\")\n",
    "except Exception as e:\n",
    "    data_processamento = \"Parâmetro não fornecido\"\n",
    "    print(f\"Erro ao obter parâmetro 'data_processamento': {e}. Usando valor padrão: {data_processamento}\")\n",
    "    # Para execução interativa (fora do Job), você pode descomentar a linha abaixo\n",
    "    # data_processamento = dbutils.widgets.get(\"data_processamento\")\n",
    "\n",
    "# Cria um DataFrame simples\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]\n",
    "columns = [\"Name\", \"ID\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Exibe o DataFrame (para logs do Job)\n",
    "print(\"DataFrame criado com sucesso:\")\n",
    "df.show()\n",
    "\n",
    "# Adiciona uma coluna com a data de processamento\n",
    "from pyspark.sql.functions import lit\n",
    "df_com_data = df.withColumn(\"Processing_Date\", lit(data_processamento))\n",
    "print(f\"DataFrame com 'Processing_Date' ({data_processamento}):\")\n",
    "df_com_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f4e5f5b-8565-4c7f-bc12-66e6ab6db19b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Fim da Execução\n",
    "O Job de teste concluiu a execução deste notebook."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_M4_Teste_Job_Simples",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
