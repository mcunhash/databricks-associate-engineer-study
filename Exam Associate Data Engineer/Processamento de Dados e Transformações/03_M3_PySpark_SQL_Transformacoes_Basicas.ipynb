{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3e5c174-f168-4894-92b4-efc97d93432a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Re-executar a criação do DataFrame (se você estiver em uma nova célula ou sessão)\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "data = [\n",
    "  (\"Alice\", 1, 1.70, \"Engenharia\"),\n",
    "  (\"Bob\", 2, 1.85, \"Ciência da Computação\"),\n",
    "  (\"Charlie\", 3, 1.75, \"Engenharia\"),\n",
    "  (\"David\", 4, 1.90, \"Matemática\"),\n",
    "  (\"Eve\", 5, 1.60, \"Ciência da Computação\"),\n",
    "  (\"Frank\", 6, None, \"Engenharia\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"Nome\", StringType(), True),\n",
    "  StructField(\"ID\", IntegerType(), True),\n",
    "  StructField(\"Altura\", DoubleType(), True),\n",
    "  StructField(\"Departamento\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Criar uma view temporária para usar com Spark SQL\n",
    "df.createOrReplaceTempView(\"pessoas\")\n",
    "\n",
    "print(\"DataFrame original:\")\n",
    "df.show()\n",
    "print(\"\\nView temporária 'pessoas' criada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d626c38-de49-4e9d-b53f-f91990e8a263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### a) Seleção e Renomeação de Colunas (Spark SQL):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25e5c7f8-b9f5-447a-8770-3388d7d3b10d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Selecionar apenas algumas colunas\n",
    "SELECT Nome, Departamento FROM pessoas;\n",
    "\n",
    "-- Renomear uma coluna\n",
    "SELECT Nome, ID, Altura, Departamento AS Area FROM pessoas;\n",
    "\n",
    "-- Selecionar e renomear ao mesmo tempo\n",
    "SELECT Nome AS Nome_Completo, ID, Altura AS Altura_Metros FROM pessoas;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8795770a-b824-45bf-b6ae-c24d73b086fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### b) Filtragem de Dados (Spark SQL):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78eca7a7-853e-40fa-a13a-12435a8c9e13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Filtrar por uma condição (pessoas da Engenharia)\n",
    "SELECT * FROM pessoas WHERE Departamento = 'Engenharia';\n",
    "\n",
    "-- Filtrar por múltiplas condições (ID > 3 E Altura > 1.70)\n",
    "SELECT * FROM pessoas WHERE ID > 3 AND Altura > 1.70;\n",
    "\n",
    "-- Filtrar por Altura > 1.80\n",
    "SELECT * FROM pessoas WHERE Altura > 1.80;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df43c92a-4ee6-4cae-bdc8-1f78ea1b799b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### c) Criação e Modificação de Colunas (Spark SQL):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed3c71e3-1914-4113-a191-ea41e6b9fb9f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762861675419}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Adicionar uma nova coluna com valor constante\n",
    "--SELECT *, 'Brasil' AS Pais FROM pessoas;\n",
    "\n",
    "-- Criar uma nova coluna baseada em uma condição (ex: 'Status_Altura')\n",
    "/*SELECT\n",
    "*,\n",
    "CASE\n",
    "  WHEN Altura >= 1.80 THEN 'Alto'\n",
    "  WHEN Altura < 1.70 THEN 'Baixo'\n",
    "  ELSE 'Medio'\n",
    "END AS Status_Altura\n",
    "FROM pessoas;*/\n",
    "\n",
    "-- Modificar uma coluna existente (ex: Altura em cm)\n",
    "SELECT Nome, ID, Altura * 100 AS Altura_cm, Departamento FROM pessoas;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "260b17c9-a97c-4b11-89d0-18d4c0c67548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### d) Agregações (Spark SQL):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0817c564-3b18-47af-8735-8e75fe103e81",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762861754478}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Contar o número de pessoas por departamento\n",
    "--SELECT Departamento, COUNT(*) AS Contagem FROM pessoas GROUP BY Departamento;\n",
    "\n",
    "-- Calcular a média da altura por departamento\n",
    "--SELECT Departamento, AVG(Altura) AS Media_Altura FROM pessoas GROUP BY Departamento;\n",
    "\n",
    "-- Múltiplas agregações\n",
    "SELECT\n",
    "Departamento,\n",
    "AVG(Altura) AS Media_Altura,\n",
    "MIN(Altura) AS Min_Altura,\n",
    "MAX(Altura) AS Max_Altura\n",
    "FROM pessoas\n",
    "GROUP BY Departamento;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4db63f48-0c00-45a4-9e9a-2952cb307809",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### e) Ordenação (Spark SQL):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14aca20c-b0b3-491d-8ff5-8331ccb11879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Ordenar por Altura (crescente)\n",
    "SELECT * FROM pessoas ORDER BY Altura ASC;\n",
    "\n",
    "-- Ordenar por Departamento (crescente) e depois por Altura (decrescente)\n",
    "SELECT * FROM pessoas ORDER BY Departamento ASC, Altura DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e384a75-001c-451c-81f8-316767961b67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### f) Remoção de Duplicatas (Spark SQL):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4847045-cd4d-45de-bebd-b3eab1833727",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adicionar um dado duplicado para demonstração\n",
    "data_duplicado = data + [(\"Alice\", 1, 1.70, \"Engenharia\")]\n",
    "df_com_duplicata = spark.createDataFrame(data_duplicado, schema)\n",
    "df_com_duplicata.createOrReplaceTempView(\"pessoas_com_duplicata\")\n",
    "\n",
    "print(\"\\nDataFrame com duplicata (para SQL):\")\n",
    "df_com_duplicata.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaea54c0-0481-4516-8249-263bce0983ba",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762862035375}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Remover linhas duplicadas (considera todas as colunas)\n",
    "--SELECT DISTINCT * FROM pessoas_com_duplicata;\n",
    "\n",
    "-- Remover duplicatas com base em um subconjunto de colunas (ex: Nome e Departamento)\n",
    "-- Isso é um pouco mais complexo em SQL puro sem funções de janela, mas pode ser feito com GROUP BY\n",
    "-- ou com funções de janela (ROW_NUMBER) para selecionar a primeira ocorrência.\n",
    "-- Usando GROUP BY para simular a remoção de duplicatas em um subconjunto:\n",
    "/*SELECT Nome, ID, Altura, Departamento\n",
    "FROM (\n",
    "SELECT *,\n",
    "       ROW_NUMBER() OVER (PARTITION BY Nome, Departamento ORDER BY ID) as rn\n",
    "FROM pessoas_com_duplicata\n",
    ")\n",
    "WHERE rn = 1;*/\n",
    "\n",
    "-- Ou, se você quer apenas as colunas Nome e Departamento sem duplicatas:\n",
    "SELECT DISTINCT Nome, Departamento FROM pessoas_com_duplicata;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8477f273-b42b-4d43-b07c-19410169bbfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### g) Tratamento de Valores Nulos (Spark SQL):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db579230-ffe2-476d-ba59-c24447d7a8b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- DataFrame com valor nulo (Frank tem Altura nula)\n",
    "-- A view 'pessoas' já tem o valor nulo para Frank.\n",
    "\n",
    "-- Remover linhas com qualquer valor nulo\n",
    "--SELECT * FROM pessoas WHERE Altura IS NOT NULL AND Nome IS NOT NULL AND ID IS NOT NULL AND Departamento IS NOT NULL;\n",
    "-- Ou, mais conciso se você sabe quais colunas podem ter nulos:\n",
    "--SELECT * FROM pessoas WHERE Altura IS NOT NULL;\n",
    "\n",
    "-- Preencher valores nulos em uma coluna específica\n",
    "/*SELECT\n",
    "Nome,\n",
    "ID,\n",
    "COALESCE(Altura, 0.0) AS Altura, -- Substitui NULL por 0.0 na coluna Altura\n",
    "Departamento\n",
    "FROM pessoas;*/\n",
    "\n",
    "-- Preencher valores nulos em todas as colunas do mesmo tipo (ex: Departamento)\n",
    "SELECT\n",
    "Nome,\n",
    "ID,\n",
    "Altura,\n",
    "COALESCE(Departamento, 'Desconhecido') AS Departamento\n",
    "FROM pessoas;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ed915b4-1c7b-4140-8afb-4fb04e710a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo de como executar SQL e obter um DataFrame PySpark\n",
    "df_engenharia_sql = spark.sql(\"SELECT * FROM pessoas WHERE Departamento = 'Engenharia'\")\n",
    "print(\"\\nResultado da query SQL em um DataFrame PySpark:\")\n",
    "df_engenharia_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0f113fc-0c9d-4bdf-9aa9-48e9eadb8c76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar tipos de dados do PySpark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# DataFrame de Pessoas (usando os dados anteriores)\n",
    "data_pessoas = [\n",
    "(\"Alice\", 1, 1.70, \"Engenharia\"),\n",
    "(\"Bob\", 2, 1.85, \"Ciência da Computação\"),\n",
    "(\"Charlie\", 3, 1.75, \"Engenharia\"),\n",
    "(\"David\", 4, 1.90, \"Matemática\"),\n",
    "(\"Eve\", 5, 1.60, \"Ciência da Computação\"),\n",
    "(\"Frank\", 6, None, \"Engenharia\"),\n",
    "(\"Grace\", 7, 1.68, \"Marketing\") # Adicionando um novo para demonstração de joins\n",
    "]\n",
    "\n",
    "schema_pessoas = StructType([\n",
    "StructField(\"Nome\", StringType(), True),\n",
    "StructField(\"ID\", IntegerType(), True),\n",
    "StructField(\"Altura\", DoubleType(), True),\n",
    "StructField(\"Departamento\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_pessoas = spark.createDataFrame(data_pessoas, schema_pessoas)\n",
    "df_pessoas.createOrReplaceTempView(\"pessoas\") # Criar view temporária para SQL\n",
    "\n",
    "print(\"DataFrame de Pessoas:\")\n",
    "df_pessoas.show()\n",
    "\n",
    "# DataFrame de Departamentos\n",
    "data_departamentos = [\n",
    "(\"Engenharia\", \"ENG\", \"São Paulo\"),\n",
    "(\"Ciência da Computação\", \"COMP\", \"Rio de Janeiro\"),\n",
    "(\"Matemática\", \"MAT\", \"Belo Horizonte\"),\n",
    "(\"Recursos Humanos\", \"RH\", \"São Paulo\") # Departamento que não tem pessoas ainda\n",
    "]\n",
    "\n",
    "schema_departamentos = StructType([\n",
    "StructField(\"Nome_Departamento\", StringType(), True),\n",
    "StructField(\"Sigla_Departamento\", StringType(), True),\n",
    "StructField(\"Localizacao\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_departamentos = spark.createDataFrame(data_departamentos, schema_departamentos)\n",
    "df_departamentos.createOrReplaceTempView(\"departamentos\") # Criar view temporária para SQL\n",
    "\n",
    "print(\"\\nDataFrame de Departamentos:\")\n",
    "df_departamentos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10949c4c-6c71-4992-96bb-7c97ed2b240c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### a) INNER JOIN (Junção Interna)\n",
    "\n",
    "O que faz: Retorna apenas as linhas onde há correspondência em ambos os DataFrames/tabelas.\n",
    "Quando usar: Quando você só quer os registros que têm informações em todas as fontes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9606025-a41a-429a-a194-2a409af50454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# INNER JOIN\n",
    "df_inner_join = df_pessoas.join(df_departamentos, df_pessoas.Departamento == df_departamentos.Nome_Departamento, \"inner\")\n",
    "print(\"\\nINNER JOIN:\")\n",
    "df_inner_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff5c9e7d-7231-4331-bced-2b8cbc7223cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- INNER JOIN\n",
    "SELECT p.Nome, p.Departamento, d.Sigla_Departamento, d.Localizacao\n",
    "FROM pessoas p\n",
    "INNER JOIN departamentos d ON p.Departamento = d.Nome_Departamento;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a85860b5-2b86-4675-9004-ff99553062f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###b) LEFT (OUTER) JOIN (Junção Externa Esquerda)\n",
    "\n",
    "O que faz: Retorna todas as linhas do DataFrame/tabela da esquerda e as linhas correspondentes do DataFrame/tabela da direita. Se não houver correspondência na direita, os valores serão null.\n",
    "Quando usar: Quando você quer manter todos os registros da tabela principal (esquerda) e adicionar informações da tabela secundária (direita, se houver)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75e8a5fc-bfc0-4da3-94cf-ae8d9069ad0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# LEFT JOIN\n",
    "df_left_join = df_pessoas.join(df_departamentos, df_pessoas.Departamento == df_departamentos.Nome_Departamento, \"left_outer\")\n",
    "print(\"\\nLEFT JOIN:\")\n",
    "df_left_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b4e6e98-26f1-4884-8547-269caefe7650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- LEFT JOIN\n",
    "SELECT p.Nome, p.Departamento, d.Sigla_Departamento, d.Localizacao\n",
    "FROM pessoas p\n",
    "LEFT JOIN departamentos d ON p.Departamento = d.Nome_Departamento;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ca78ee7-866c-4be4-a699-972b4e4fe8fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### c) RIGHT (OUTER) JOIN (Junção Externa Direita)\n",
    "\n",
    "O que faz: Retorna todas as linhas do DataFrame/tabela da direita e as linhas correspondentes do DataFrame/tabela da esquerda. Se não houver correspondência na esquerda, os valores serão null.\n",
    "Quando usar: Quando você quer manter todos os registros da tabela secundária (direita) e adicionar informações da tabela principal (esquerda, se houver)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd12039-10da-4527-88b1-91f5542be509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RIGHT JOIN\n",
    "df_right_join = df_pessoas.join(df_departamentos, df_pessoas.Departamento == df_departamentos.Nome_Departamento, \"right_outer\")\n",
    "print(\"\\nRIGHT JOIN:\")\n",
    "df_right_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cbbdf75-7f5e-4733-b25a-322d4c96d49d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- RIGHT JOIN\n",
    "SELECT p.Nome, p.Departamento, d.Sigla_Departamento, d.Localizacao\n",
    "FROM pessoas p\n",
    "RIGHT JOIN departamentos d ON p.Departamento = d.Nome_Departamento;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30571e3b-4d2f-41fe-974b-31af096eb089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### d) FULL (OUTER) JOIN (Junção Externa Completa)\n",
    "\n",
    "O que faz: Retorna todas as linhas de ambos os DataFrames/tabelas. Se não houver correspondência em um dos lados, os valores serão null para as colunas desse lado.\n",
    "Quando usar: Quando você quer ver todos os registros de ambas as tabelas, independentemente de haver correspondência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea15546-7483-4543-b24e-22b47d745a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FULL JOIN\n",
    "df_full_join = df_pessoas.join(df_departamentos, df_pessoas.Departamento == df_departamentos.Nome_Departamento, \"full_outer\")\n",
    "print(\"\\nFULL JOIN:\")\n",
    "df_full_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7062bdf-9497-4434-8fde-e535c6f4e35f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- FULL JOIN\n",
    "SELECT p.Nome, p.Departamento, d.Sigla_Departamento, d.Localizacao\n",
    "FROM pessoas p\n",
    "FULL JOIN departamentos d ON p.Departamento = d.Nome_Departamento;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2140f67-cd6a-4eda-bf7c-36c24f44e51b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### e) ANTI JOIN (Junção Anti)\n",
    "\n",
    "O que faz: Retorna as linhas do DataFrame/tabela da esquerda que não têm correspondência no DataFrame/tabela da direita. É o oposto de um INNER JOIN para o lado esquerdo.\n",
    "Quando usar: Para encontrar registros \"órfãos\" ou identificar dados que não existem em outra tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "467cd361-69eb-4c85-a9e5-bf455a846dd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ANTI JOIN\n",
    "df_anti_join = df_pessoas.join(df_departamentos, df_pessoas.Departamento == df_departamentos.Nome_Departamento, \"left_anti\")\n",
    "print(\"\\nANTI JOIN (Pessoas sem Departamento correspondente na tabela de Departamentos):\")\n",
    "df_anti_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83160c3d-65e4-4223-9e73-e9512ba94638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ANTI JOIN (usando NOT EXISTS ou LEFT JOIN com WHERE IS NULL)\n",
    "-- Opção 1: Usando NOT EXISTS\n",
    "SELECT p.Nome, p.Departamento\n",
    "FROM pessoas p\n",
    "WHERE NOT EXISTS (\n",
    "SELECT 1 FROM departamentos d WHERE p.Departamento = d.Nome_Departamento\n",
    ");\n",
    "\n",
    "-- Opção 2: Usando LEFT JOIN com WHERE IS NULL (comum para simular ANTI JOIN)\n",
    "SELECT p.Nome, p.Departamento\n",
    "FROM pessoas p\n",
    "LEFT JOIN departamentos d ON p.Departamento = d.Nome_Departamento\n",
    "WHERE d.Nome_Departamento IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0faf84da-89f0-4061-89d8-9221cc8eb6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Tópico: Funções de Janela (Window Functions)\n",
    "\n",
    "O que são Funções de Janela?\n",
    "\n",
    "Ao contrário das funções de agregação (SUM, AVG, COUNT) que colapsam linhas em um único resultado por grupo (GROUP BY), as funções de janela retornam um valor para cada linha do DataFrame, mas esse valor é calculado com base em um \"grupo\" de linhas relacionadas (a janela).\n",
    "\n",
    "**Componentes** Chave de uma Função de Janela:\n",
    "\n",
    "Função de Janela: A função de agregação ou analítica a ser aplicada (ex: ROW_NUMBER(), RANK(), LAG(), SUM(), AVG()).\n",
    "`OVER() Clause`: Define a janela sobre a qual a função será aplicada. Ela tem três partes principais:\n",
    "`PARTITION BY`: Divide os dados em partições (grupos) independentes. A função de janela é aplicada separadamente dentro de cada partição. (Similar ao GROUP BY, mas não colapsa as linhas).\n",
    "`ORDER BY`: Define a ordem das linhas dentro de cada partição. Isso é crucial para funções que dependem da ordem, como ranking, LAG, LEAD, ou somas cumulativas.\n",
    "`ROWS BETWEEN / RANGE BETWEEN`: Define os limites da janela dentro de cada partição. Por exemplo, \"as 3 linhas anteriores e a linha atual\" (ROWS BETWEEN 3 PRECEDING AND CURRENT ROW). Se omitido, geralmente a janela padrão é do início da partição até a linha atual (RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) ou toda a partição (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING), dependendo da função e do contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c028b53c-c835-4b7b-968b-cdf734487adb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Re-executar a criação do DataFrame de Pessoas (se você estiver em uma nova célula ou sessão)\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "data_pessoas = [\n",
    "  (\"Alice\", 1, 1.70, \"Engenharia\", 7000),\n",
    "  (\"Bob\", 2, 1.85, \"Ciência da Computação\", 8500),\n",
    "  (\"Charlie\", 3, 1.75, \"Engenharia\", 7500),\n",
    "  (\"David\", 4, 1.90, \"Matemática\", 9000),\n",
    "  (\"Eve\", 5, 1.60, \"Ciência da Computação\", 8000),\n",
    "  (\"Frank\", 6, None, \"Engenharia\", 6000),\n",
    "  (\"Grace\", 7, 1.68, \"Marketing\", 6500),\n",
    "  (\"Heidi\", 8, 1.72, \"Engenharia\", 7000) # Adicionando mais um para ranking\n",
    "]\n",
    "\n",
    "schema_pessoas = StructType([\n",
    "  StructField(\"Nome\", StringType(), True),\n",
    "  StructField(\"ID\", IntegerType(), True),\n",
    "  StructField(\"Altura\", DoubleType(), True),\n",
    "  StructField(\"Departamento\", StringType(), True),\n",
    "  StructField(\"Salario\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_pessoas_salario = spark.createDataFrame(data_pessoas, schema_pessoas)\n",
    "df_pessoas_salario.createOrReplaceTempView(\"pessoas_salario\") # Criar view temporária para SQL\n",
    "\n",
    "print(\"DataFrame de Pessoas com Salário:\")\n",
    "df_pessoas_salario.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ccbe31d-dab2-41ae-9b99-e031bcb6526f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Exemplos de Funções de Janela\n",
    "\n",
    "a) Funções de Ranking (ROW_NUMBER, RANK, DENSE_RANK, NTILE)\n",
    "\n",
    "ROW_NUMBER(): Atribui um número sequencial único a cada linha dentro de sua partição, começando em 1.\n",
    "RANK(): Atribui um rank a cada linha dentro de sua partição. Se houver empates, as linhas empatadas recebem o mesmo rank, e o próximo rank é \"pulado\".\n",
    "DENSE_RANK(): Similar ao RANK(), mas não pula ranks em caso de empates.\n",
    "NTILE(n): Divide as linhas em n grupos (tiles) de tamanho aproximadamente igual e atribui um número de grupo a cada linha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aac5ee80-bf8c-4d2e-a357-988c8a80e080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, ntile, col\n",
    "\n",
    "# Definir a especificação da janela: Particionar por Departamento, ordenar por Salario (decrescente)\n",
    "window_spec = Window.partitionBy(\"Departamento\").orderBy(col(\"Salario\").desc())\n",
    "\n",
    "# Aplicar funções de ranking\n",
    "df_ranking = df_pessoas_salario.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "                             .withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "                             .withColumn(\"dense_rank\", dense_rank().over(window_spec)) \\\n",
    "                             .withColumn(\"ntile_2\", ntile(2).over(window_spec)) # Dividir em 2 grupos\n",
    "\n",
    "print(\"\\nFunções de Ranking por Departamento (Salário Decrescente):\")\n",
    "df_ranking.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950469e6-907d-4203-8bb9-70b5a517e268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Funções de Ranking por Departamento (Salário Decrescente)\n",
    "SELECT\n",
    "Nome,\n",
    "Departamento,\n",
    "Salario,\n",
    "ROW_NUMBER() OVER (PARTITION BY Departamento ORDER BY Salario DESC) AS row_num,\n",
    "RANK() OVER (PARTITION BY Departamento ORDER BY Salario DESC) AS rank,\n",
    "DENSE_RANK() OVER (PARTITION BY Departamento ORDER BY Salario DESC) AS dense_rank,\n",
    "NTILE(2) OVER (PARTITION BY Departamento ORDER BY Salario DESC) AS ntile_2\n",
    "FROM pessoas_salario;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "527a1566-5244-4455-905d-356ba2ac791e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### b) Funções de Deslocamento (LAG, LEAD)\n",
    "\n",
    "LAG(coluna, offset, default): Retorna o valor de uma coluna de uma linha anterior dentro da mesma partição.\n",
    "LEAD(coluna, offset, default): Retorna o valor de uma coluna de uma linha posterior dentro da mesma partição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5d2f5a0-e280-4761-8fa7-4414613ff083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag, lead\n",
    "\n",
    "# Definir a especificação da janela: Particionar por Departamento, ordenar por Salario (crescente)\n",
    "window_spec_order_salario = Window.partitionBy(\"Departamento\").orderBy(\"Salario\")\n",
    "\n",
    "# Aplicar funções LAG e LEAD\n",
    "df_lag_lead = df_pessoas_salario.withColumn(\"salario_anterior\", lag(\"Salario\", 1).over(window_spec_order_salario)) \\\n",
    "                              .withColumn(\"salario_proximo\", lead(\"Salario\", 1).over(window_spec_order_salario))\n",
    "\n",
    "print(\"\\nFunções LAG e LEAD (Salário Anterior/Próximo por Departamento):\")\n",
    "df_lag_lead.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a759ea9-7dd6-4e17-be87-3506ac79ff21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Funções LAG e LEAD (Salário Anterior/Próximo por Departamento)\n",
    "SELECT\n",
    "Nome,\n",
    "Departamento,\n",
    "Salario,\n",
    "LAG(Salario, 1) OVER (PARTITION BY Departamento ORDER BY Salario) AS salario_anterior,\n",
    "LEAD(Salario, 1) OVER (PARTITION BY Departamento ORDER BY Salario) AS salario_proximo\n",
    "FROM pessoas_salario;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6667edd2-780c-46fd-8d49-8873bb9b0188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### c) Agregações de Janela (Soma Cumulativa, Média Móvel)\n",
    "\n",
    "Você pode usar funções de agregação como SUM, AVG, COUNT, MIN, MAX como funções de janela, definindo os limites da janela com ROWS BETWEEN ou RANGE BETWEEN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a465272d-5f8c-49c2-a82f-8a4b2f344911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg\n",
    "\n",
    "# Definir a especificação da janela para soma cumulativa: Particionar por Departamento, ordenar por Salario\n",
    "# A janela vai do início da partição até a linha atual\n",
    "window_spec_cumulative = Window.partitionBy(\"Departamento\").orderBy(\"Salario\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Definir a especificação da janela para média móvel: Particionar por Departamento, ordenar por Salario\n",
    "# A janela vai da linha anterior até a linha atual\n",
    "window_spec_moving_avg = Window.partitionBy(\"Departamento\").orderBy(\"Salario\").rowsBetween(-1, Window.currentRow)\n",
    "\n",
    "# Aplicar soma cumulativa e média móvel\n",
    "df_cumulative_moving = df_pessoas_salario.withColumn(\"salario_acumulado\", sum(\"Salario\").over(window_spec_cumulative)) \\\n",
    "                                       .withColumn(\"media_movel_salario\", avg(\"Salario\").over(window_spec_moving_avg))\n",
    "\n",
    "print(\"\\nSoma Cumulativa e Média Móvel de Salário por Departamento:\")\n",
    "df_cumulative_moving.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5243d1e2-fe54-4edd-b6d3-da204dbe2bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Soma Cumulativa e Média Móvel de Salário por Departamento\n",
    "SELECT\n",
    "Nome,\n",
    "Departamento,\n",
    "Salario,\n",
    "SUM(Salario) OVER (PARTITION BY Departamento ORDER BY Salario ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS salario_acumulado,\n",
    "AVG(Salario) OVER (PARTITION BY Departamento ORDER BY Salario ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS media_movel_salario\n",
    "FROM pessoas_salario;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6db6164b-4505-43c1-b55a-324b02ceec1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Tópico: Trabalhando com Tipos de Dados Complexos (Arrays, Structs, Maps)\n",
    "\n",
    "No mundo real, os dados raramente são \"planos\" (**flat**). É muito comum encontrar dados aninhados ou semi-estruturados, especialmente ao lidar com fontes como **JSON**, **XML** ou **logs**. O Spark, através de seus DataFrames, oferece suporte robusto para trabalhar com esses tipos de dados complexos: `Structs`, `Arrays` e `Maps`.\n",
    "\n",
    "Compreender como manipular esses tipos é crucial para extrair informações valiosas e transformar dados para análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d85b9e2c-f2fb-470a-83c7-4c10ac7fa624",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType\n",
    "from pyspark.sql.functions import col, explode, posexplode, map_keys, map_values, lit, array_contains, struct, create_map\n",
    "\n",
    "# Dados de exemplo com tipos complexos\n",
    "data_complexos = [\n",
    "  (\"Alice\", 1, {\"rua\": \"Rua A\", \"numero\": 10}, [\"Python\", \"SQL\"], {\"projeto1\": \"ativo\", \"projeto2\": \"concluido\"}),\n",
    "  (\"Bob\", 2, {\"rua\": \"Av. B\", \"numero\": 25}, [\"Java\", \"Spark\", \"SQL\"], {\"projeto3\": \"ativo\"}),\n",
    "  (\"Charlie\", 3, {\"rua\": \"Rua C\", \"numero\": 5}, [\"Python\", \"Scala\"], {}),\n",
    "  (\"David\", 4, None, [\"C++\"], {\"projeto4\": \"pendente\", \"projeto5\": \"ativo\"}), # Exemplo com struct nulo\n",
    "  (\"Eve\", 5, {\"rua\": \"Trav. D\", \"numero\": 12}, [], {\"projeto6\": \"concluido\"}) # Exemplo com array vazio\n",
    "]\n",
    "\n",
    "# Definir o esquema (schema) do DataFrame\n",
    "schema_complexos = StructType([\n",
    "  StructField(\"Nome\", StringType(), True),\n",
    "  StructField(\"ID\", IntegerType(), True),\n",
    "  StructField(\"Endereco\", StructType([\n",
    "      StructField(\"rua\", StringType(), True),\n",
    "      StructField(\"numero\", IntegerType(), True)\n",
    "  ]), True),\n",
    "  StructField(\"Habilidades\", ArrayType(StringType()), True),\n",
    "  StructField(\"StatusProjetos\", MapType(StringType(), StringType()), True)\n",
    "])\n",
    "\n",
    "df_complexos = spark.createDataFrame(data_complexos, schema_complexos)\n",
    "df_complexos.createOrReplaceTempView(\"dados_complexos\") # Criar view temporária para SQL\n",
    "\n",
    "print(\"DataFrame com Tipos Complexos:\")\n",
    "df_complexos.printSchema()\n",
    "df_complexos.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bc3813f-f45c-4e4b-aaa2-feaddbacf4d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Manipulando Structs (Registros Aninhados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf971bde-a05e-4e9b-b199-ccd971c05b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### a) Acessando Campos de um Struct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaba44d0-54ba-4f6f-97b2-8a3ff4f87dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Acessar campos específicos do struct 'Endereco'\n",
    "df_struct_acesso = df_complexos.select(\"Nome\", \"Endereco.rua\", col(\"Endereco.numero\").alias(\"num_casa\"))\n",
    "print(\"\\nAcessando campos de Struct:\")\n",
    "df_struct_acesso.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af2844c1-1ae8-4f0c-9ff0-e6933e6be309",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Acessar campos específicos do struct 'Endereco'\n",
    "SELECT Nome, Endereco.rua, Endereco.numero AS num_casa\n",
    "FROM dados_complexos;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d811227-ed6a-4999-91e2-ecccedc1dd05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### b) Criando um Novo Struct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "739c9f77-1be4-4a44-b8f9-23c197dfd971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criar um novo struct a partir de colunas existentes\n",
    "df_novo_struct = df_complexos.withColumn(\"InfoPessoal\", struct(col(\"Nome\").alias(\"nome_completo\"), col(\"ID\").alias(\"identificador\")))\n",
    "print(\"\\nCriando um novo Struct:\")\n",
    "df_novo_struct.printSchema()\n",
    "df_novo_struct.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c092d3bc-5619-47c2-ac33-1e1a400997f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Criar um novo struct a partir de colunas existentes\n",
    "SELECT\n",
    "Nome,\n",
    "ID,\n",
    "Endereco,\n",
    "Habilidades,\n",
    "StatusProjetos,\n",
    "STRUCT(Nome AS nome_completo, ID AS identificador) AS InfoPessoal\n",
    "FROM dados_complexos;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5758a55-cbb8-49b9-8f67-9085471ce0ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Manipulando Arrays (Listas de Elementos)\n",
    "Um ArrayType é uma lista de elementos do mesmo tipo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30923cbd-e3c9-4801-bd5d-a52225e10b2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### a) Explodindo Arrays (explode, posexplode):\n",
    "\n",
    "- explode: Transforma cada elemento de um array em uma nova linha. Se o array for nulo ou vazio, a linha original é descartada.\n",
    "\n",
    "- posexplode: Similar ao explode, mas também adiciona uma coluna com a posição (índice) do elemento no array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "283fc029-7592-4f58-84b5-400af8a7d957",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explodir o array 'Habilidades'\n",
    "df_explode = df_complexos.select(\"Nome\", explode(\"Habilidades\").alias(\"Habilidade_Individual\"))\n",
    "print(\"\\nExplodindo Array (explode):\")\n",
    "df_explode.show(truncate=False)\n",
    "\n",
    "# Explodir o array 'Habilidades' com posição\n",
    "df_posexplode = df_complexos.select(\"Nome\", posexplode(\"Habilidades\").alias(\"posicao\", \"Habilidade_Individual\"))\n",
    "print(\"\\nExplodindo Array com Posição (posexplode):\")\n",
    "df_posexplode.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d40c64a-2784-485d-8533-230fefac8f5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Explodir o array 'Habilidades'\n",
    "SELECT Nome, Habilidade_Individual\n",
    "FROM dados_complexos\n",
    "LATERAL VIEW explode(Habilidades) AS Habilidade_Individual;\n",
    "\n",
    "-- Explodir o array 'Habilidades' com posição\n",
    "SELECT Nome, posicao, Habilidade_Individual\n",
    "FROM dados_complexos\n",
    "LATERAL VIEW posexplode(Habilidades) AS posicao, Habilidade_Individual;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f50ce693-1aad-4728-b5c5-3a0a01531b93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### b) Filtrando por Conteúdo do Array (array_contains):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b313796-b064-440c-bed2-1eb649b7bd81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtrar pessoas que possuem a habilidade 'Python'\n",
    "df_python_skills = df_complexos.filter(array_contains(col(\"Habilidades\"), \"Python\"))\n",
    "print(\"\\nPessoas com habilidade 'Python':\")\n",
    "df_python_skills.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3947881d-d242-4c85-b282-65dc64d590d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Filtrar pessoas que possuem a habilidade 'Python'\n",
    "SELECT *\n",
    "FROM dados_complexos\n",
    "WHERE array_contains(Habilidades, 'Python');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "418bc81c-b12d-48fa-8ff0-d91d3ac26f10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Manipulando Maps (Pares Chave-Valor)\n",
    "\n",
    "Um MapType é uma coleção de pares chave-valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19805dce-8a14-4d78-b88c-96f9327e2817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### a) Acessando Valores por Chave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d3bff7-d780-4cfc-a8de-26e4ef86b5e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Acessar o status de um projeto específico (ex: 'projeto1')\n",
    "df_map_acesso = df_complexos.select(\"Nome\", col(\"StatusProjetos\")[\"projeto1\"].alias(\"Status_Projeto1\"))\n",
    "print(\"\\nAcessando valor de Map por chave:\")\n",
    "df_map_acesso.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d097e5f-d38e-40cb-a241-9c06e7f8f3cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Acessar o status de um projeto específico (ex: 'projeto1')\n",
    "SELECT Nome, StatusProjetos['projeto1'] AS Status_Projeto1\n",
    "FROM dados_complexos;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "933db25d-59be-4778-a494-df215cc1045d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### b) Obtendo Chaves e Valores do Map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a099dd6-9342-464e-b55b-6e4e506b3395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obter todas as chaves e valores do map\n",
    "df_map_keys_values = df_complexos.select(\"Nome\", map_keys(\"StatusProjetos\").alias(\"Chaves_Projetos\"), map_values(\"StatusProjetos\").alias(\"Valores_Projetos\"))\n",
    "print(\"\\nObtendo Chaves e Valores de Map:\")\n",
    "df_map_keys_values.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "611d6265-f89d-469f-999c-9a851f548acb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Obter todas as chaves e valores do map\n",
    "SELECT Nome, map_keys(StatusProjetos) AS Chaves_Projetos, map_values(StatusProjetos) AS Valores_Projetos\n",
    "FROM dados_complexos;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57e5deb4-c27e-4ca7-a87c-3e9f35a8a886",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### c) Explodindo Maps (explode):\n",
    "\n",
    "Assim como arrays, maps também podem ser \"explodidos\" para transformar cada par chave-valor em uma nova linha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6cd067f-5491-42fd-80fa-74022a3d95be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explodir o map 'StatusProjetos'\n",
    "df_explode_map = df_complexos.select(\"Nome\", explode(\"StatusProjetos\").alias(\"Projeto\", \"Status\"))\n",
    "print(\"\\nExplodindo Map (explode):\")\n",
    "df_explode_map.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b6d9f4d-5117-40c1-b2c5-e441be8f432a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Explodir o map 'StatusProjetos'\n",
    "SELECT Nome, Projeto, Status\n",
    "FROM dados_complexos\n",
    "LATERAL VIEW explode(StatusProjetos) AS Projeto, Status;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5037539466847789,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_M3_PySpark_SQL_Transformacoes_Basicas",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
